{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69512e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "import os\n",
    "from langsmith import traceable\n",
    "import json\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34849678",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_KEY=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "AZURE_OPENAI_ENDPOINT=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "AZURE_OPENAI_API_VERSION=os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "LLM_MODEL2 = \"gpt-4.1-mini\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b1dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class GenerateAnswer(BaseModel):\n",
    "    generated_answer: str = Field(description=\"The answer generated\")\n",
    "\n",
    "class Critique(BaseModel):\n",
    "    issues: List[str] = Field(description=\"Problems in the answer\")\n",
    "    score: int = Field(description=\"Quality score 1-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95c1c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel()\n",
    "\n",
    "service = AzureChatCompletion(\n",
    "    deployment_name=LLM_MODEL2,\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "kernel.add_service(service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a43e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_prompt = \"\"\"\n",
    "Answer the user's question clearly and in detail within 150 words.\n",
    "\n",
    "Question: {{$input}}\n",
    "\"\"\"\n",
    "request_settings_generator = AzureChatPromptExecutionSettings(temperature=0,max_tokens=500,\n",
    "                                                           response_format=GenerateAnswer)\n",
    "generator_function = kernel.add_function(plugin_name=\"reflect\", function_name=\"generator\", \n",
    "                                         prompt=generator_prompt,\n",
    "                                         prompt_execution_settings=request_settings_generator)\n",
    "\n",
    "@traceable(name=\"Generate Step\")\n",
    "async def run_generator(q):\n",
    "    generator_response = await kernel.invoke(generator_function, input=q)\n",
    "    return GenerateAnswer.model_validate(json.loads(generator_response.value[0].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7edc6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_response = await kernel.invoke(generator_function, input=\"Explain how hybrid retrieval improves RAG systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a338481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_response.value[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29ac3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rspns = await run_generator(\"Explain how hybrid retrieval improves RAG systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357ad330",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_prompt = \"\"\"\n",
    "You are a strict reviewer.\n",
    "\n",
    "Evaluate the answer for:\n",
    "- correctness\n",
    "- missing details\n",
    "- clarity\n",
    "\n",
    "Answer:\n",
    "{{$answer}}\n",
    "\n",
    "Return JSON:\n",
    "- issues (list)\n",
    "- score (1-10)\n",
    "\"\"\"\n",
    "request_settings_critic = AzureChatPromptExecutionSettings(temperature=0,max_tokens=500,\n",
    "                                                           response_format=Critique)\n",
    "critic_function = kernel.add_function(\n",
    "    plugin_name=\"reflect\",\n",
    "    function_name=\"critic\", \n",
    "    prompt=critic_prompt, \n",
    "    prompt_execution_settings=request_settings_critic\n",
    ")\n",
    "\n",
    "@traceable(name=\"Critic Step\")\n",
    "async def run_critic(answer):\n",
    "    result = await kernel.invoke(critic_function, answer=str(answer.generated_answer))\n",
    "    return Critique.model_validate(json.loads(result.value[0].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85ead930",
   "metadata": {},
   "outputs": [],
   "source": [
    "refiner_prompt = \"\"\"\n",
    "Improve the original answer using the critique.\n",
    "\n",
    "Original Answer:\n",
    "{{$answer}}\n",
    "\n",
    "Issues Found:\n",
    "{{$issues}}\n",
    "\n",
    "Provide an improved version.\n",
    "\"\"\"\n",
    "request_settings_refiner = AzureChatPromptExecutionSettings(temperature=0,max_tokens=500,\n",
    "                                                           response_format=GenerateAnswer)\n",
    "refiner_function = kernel.add_function(plugin_name=\"reflect\", function_name=\"refiner\",\n",
    "                                       prompt=refiner_prompt,\n",
    "                                       prompt_execution_settings=request_settings_refiner)\n",
    "\n",
    "@traceable(name=\"Refiner Step\")\n",
    "async def run_refiner(answer, critique):\n",
    "    final_result = await kernel.invoke(\n",
    "        refiner_function,\n",
    "        answer=str(answer.generated_answer),\n",
    "        issues=\"\\n\".join(critique.issues)\n",
    "    )\n",
    "    return GenerateAnswer.model_validate(json.loads(final_result.value[0].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25ab5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(name=\"Reflection Agent\")\n",
    "async def reflection_agent(user_question):\n",
    "\n",
    "    # Step 1: Generate\n",
    "    draft = await run_generator(user_question)\n",
    "\n",
    "    # Step 2: Critique\n",
    "    critique = await run_critic(draft)\n",
    "\n",
    "    print(f\"[Critic Score] {critique.score}/10\")\n",
    "\n",
    "    # Step 3: Improve if needed\n",
    "    if critique.score < 8:\n",
    "        final_answer = await run_refiner(draft, critique)\n",
    "    else:\n",
    "        final_answer = draft\n",
    "\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd8d832c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Critic Score] 8/10\n"
     ]
    }
   ],
   "source": [
    "answer = await reflection_agent(\n",
    "    \"Explain how hybrid retrieval improves RAG systems\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad34d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid retrieval enhances Retrieval-Augmented Generation (RAG) systems by combining multiple retrieval methods, typically dense and sparse retrieval techniques, to improve the quality and relevance of retrieved documents. Sparse retrieval, like BM25, excels at exact keyword matching and is efficient for large-scale document collections, but may miss semantically relevant content. Dense retrieval uses neural embeddings to capture semantic similarity, retrieving documents that are contextually related even if they don't share exact terms. By integrating both, hybrid retrieval leverages the precision of sparse methods and the semantic understanding of dense methods, resulting in a more comprehensive and diverse set of relevant documents. This richer retrieval input enables the RAG model to generate more accurate, informative, and contextually appropriate responses, ultimately improving the system's overall performance and robustness in handling varied queries.\n"
     ]
    }
   ],
   "source": [
    "print(answer.generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa856ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82234e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82164be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac344e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb35d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd401d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
