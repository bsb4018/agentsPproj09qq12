{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6cde3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "import os\n",
    "from langsmith import traceable\n",
    "import json\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d249914",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_KEY=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "AZURE_OPENAI_ENDPOINT=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "AZURE_OPENAI_API_VERSION=os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "AZURE_LLM_MODEL=os.environ[\"AZURE_LLM_MODEL\"]\n",
    "AZURE_EMBEDDING_MODEL=os.environ[\"AZURE_EMBEDDING_MODEL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel()\n",
    "\n",
    "service = AzureChatCompletion(\n",
    "    deployment_name=AZURE_LLM_MODEL,\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "kernel.add_service(service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_term_memory = []\n",
    "\n",
    "def update_short_term(user, assistant):\n",
    "    short_term_memory.append({\"user\": user, \"assistant\": assistant})\n",
    "    return short_term_memory[-5:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49487621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "EPISODIC_FILE = \"episodic_memory.json\"\n",
    "\n",
    "def store_episode(user, response):\n",
    "    entry = {\n",
    "        \"time\": str(datetime.now()),\n",
    "        \"user\": user,\n",
    "        \"assistant\": response\n",
    "    }\n",
    "    try:\n",
    "        data = json.load(open(EPISODIC_FILE))\n",
    "    except:\n",
    "        data = []\n",
    "    data.append(entry)\n",
    "    json.dump(data, open(EPISODIC_FILE, \"w\"))\n",
    "\n",
    "def get_episodic_memory():\n",
    "    try:\n",
    "        return json.load(open(EPISODIC_FILE))[-3:]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# embedding_model = AzureOpenAIEmbeddings(\n",
    "#     azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "#     api_key=AZURE_OPENAI_API_KEY,\n",
    "#     azure_deployment=AZURE_EMBEDDING_MODEL,\n",
    "#     openai_api_version=AZURE_OPENAI_API_VERSION\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81330d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "def get_embedding(text: str):\n",
    "    response = client.embeddings.create(\n",
    "        model=AZURE_EMBEDDING_MODEL,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "424f70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "dimension = 3072\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "memory_texts = [\"RAG uses vector search\", \"Hybrid retrieval combines BM25 + vectors\"]\n",
    "\n",
    "def save_memory():\n",
    "    faiss.write_index(index, \"vector.index\")\n",
    "    with open(\"memory_texts.pkl\", \"wb\") as f:\n",
    "        pickle.dump(memory_texts, f)\n",
    "\n",
    "def load_memory():\n",
    "    global index, memory_texts\n",
    "    index = faiss.read_index(\"vector.index\")\n",
    "    memory_texts = pickle.load(open(\"memory_texts.pkl\", \"rb\"))\n",
    "\n",
    "def add_to_long_term_memory(text):\n",
    "    embedding = np.array([get_embedding(text)]).astype(\"float32\")\n",
    "    index.add(embedding)\n",
    "    memory_texts.append(text)\n",
    "\n",
    "def retrieve_long_term(query, k=3):\n",
    "    query_vector = np.array([get_embedding(query)]).astype(\"float32\")\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    return [memory_texts[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c948c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_agent_prompt = \"\"\"\n",
    "You are an intelligent assistant with memory.\n",
    "\n",
    "Use the context below to answer the user.\n",
    "\n",
    "Short-Term Memory (recent chat):\n",
    "{{$short_term}}\n",
    "\n",
    "Episodic Memory (past interactions):\n",
    "{{$episodic}}\n",
    "\n",
    "Long-Term Knowledge:\n",
    "{{$long_term}}\n",
    "\n",
    "User Question:\n",
    "{{$question}}\n",
    "\n",
    "Provide a helpful, context-aware answer.\n",
    "\"\"\"\n",
    "request_settings_memag = AzureChatPromptExecutionSettings(temperature=0.1,max_tokens=1000)\n",
    "main_agent_function = kernel.add_function(\n",
    "    plugin_name=\"memory_core\",\n",
    "    function_name=\"context_reasoner\",\n",
    "    prompt=memory_agent_prompt,\n",
    "    prompt_execution_settings=request_settings_memag \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(name=\"Memory Agent\")\n",
    "async def memory_agent(user_input):\n",
    "\n",
    "    short_context = update_short_term(user_input, \"\")\n",
    "    episodic_context = get_episodic_memory()\n",
    "    long_term_context = retrieve_long_term(user_input)\n",
    "\n",
    "    response = await kernel.invoke(\n",
    "        main_agent_function,\n",
    "        short_term=str(short_context),\n",
    "        episodic=str(episodic_context),\n",
    "        long_term=str(long_term_context),\n",
    "        question=user_input\n",
    "    )\n",
    "\n",
    "    store_episode(user_input, str(response))\n",
    "    # add_to_long_term_memory(str(response))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aca18a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = await memory_agent(\n",
    "    \"How does hybrid retrieval improve RAG systems?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2429b122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid retrieval improves Retrieval-Augmented Generation (RAG) systems by integrating traditional keyword-based methods, such as BM25, with vector-based approaches. This combination allows the system to leverage the strengths of both techniques: \n",
      "\n",
      "- **BM25** excels at retrieving documents based on exact keyword matches, ensuring that relevant documents containing specific terms are identified.\n",
      "- **Vector-based methods** capture semantic similarities, enabling the retrieval of documents that may not contain the exact keywords but are contextually relevant.\n",
      "\n",
      "By merging these two retrieval strategies, hybrid retrieval enhances the overall effectiveness and accuracy of RAG systems. This leads to improved performance in generating responses that are both relevant and informative, as the system can draw from a broader range of documents that align with the user's intent.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "077e4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = await memory_agent(\n",
    "    \"Give short idea on how to implement hybrid search in RAG systems?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "382b33f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To implement hybrid search in Retrieval-Augmented Generation (RAG) systems, you can follow these steps:\n",
      "\n",
      "1. **Data Preparation**: Ensure your document corpus is indexed for both keyword-based and vector-based retrieval. This involves preprocessing text for BM25 and generating embeddings for vector search.\n",
      "\n",
      "2. **Indexing**: Use a search engine like Elasticsearch for BM25 indexing and a vector database (e.g., FAISS or Pinecone) for storing and querying embeddings.\n",
      "\n",
      "3. **Query Processing**: When a query is received, first process it to retrieve relevant documents using both methods:\n",
      "   - **BM25 Retrieval**: Use the traditional keyword-based search to get a list of documents based on exact matches.\n",
      "   - **Vector Retrieval**: Convert the query into an embedding and retrieve documents based on semantic similarity.\n",
      "\n",
      "4. **Combining Results**: Merge the results from both retrieval methods. You can rank them based on relevance scores from BM25 and similarity scores from the vector search, or use a weighted approach to balance their contributions.\n",
      "\n",
      "5. **Response Generation**: Feed the combined results into the generative model to produce a response that leverages the strengths of both retrieval methods.\n",
      "\n",
      "By following these steps, you can effectively implement hybrid search in RAG systems, enhancing the quality and relevance of generated responses.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889cb47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b98847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb3a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918808ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
