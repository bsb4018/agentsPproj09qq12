{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d69512e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "import os\n",
    "from langsmith import traceable\n",
    "import json\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34849678",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_KEY=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "AZURE_OPENAI_ENDPOINT=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "AZURE_OPENAI_API_VERSION=os.environ[\"AZURE_OPENAI_API_VERSION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7b1dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class GenerateAnswer(BaseModel):\n",
    "    generated_answer: str = Field(description=\"The answer generated\")\n",
    "\n",
    "class Critique(BaseModel):\n",
    "    issues: List[str] = Field(description=\"Problems in the answer\")\n",
    "    score: int = Field(description=\"Quality score 1-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95c1c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel()\n",
    "\n",
    "service = AzureChatCompletion(\n",
    "    deployment_name=\"gpt-4o-mini\",\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "kernel.add_service(service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52a43e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_prompt = \"\"\"\n",
    "Answer the user's question clearly and in detail within 150 words.\n",
    "\n",
    "Question: {{$input}}\n",
    "\"\"\"\n",
    "request_settings_generator = AzureChatPromptExecutionSettings(temperature=0,max_tokens=500,\n",
    "                                                           response_format=GenerateAnswer)\n",
    "generator_function = kernel.add_function(plugin_name=\"reflect\", function_name=\"generator\", \n",
    "                                         prompt=generator_prompt,\n",
    "                                         prompt_execution_settings=request_settings_generator)\n",
    "\n",
    "@traceable(name=\"Generate Step\")\n",
    "async def run_generator(q):\n",
    "    generator_response = await kernel.invoke(generator_function, input=q)\n",
    "    return GenerateAnswer.model_validate(json.loads(generator_response.value[0].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7edc6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_response = await kernel.invoke(generator_function, input=\"Explain how hybrid retrieval improves RAG systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a338481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_response.value[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29ac3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rspns = await run_generator(\"Explain how hybrid retrieval improves RAG systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "357ad330",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_prompt = \"\"\"\n",
    "You are a strict reviewer.\n",
    "\n",
    "Evaluate the answer for:\n",
    "- correctness\n",
    "- missing details\n",
    "- clarity\n",
    "\n",
    "Answer:\n",
    "{{$answer}}\n",
    "\n",
    "Return JSON:\n",
    "- issues (list)\n",
    "- score (1-10)\n",
    "\"\"\"\n",
    "request_settings_critic = AzureChatPromptExecutionSettings(temperature=0,max_tokens=500,\n",
    "                                                           response_format=Critique)\n",
    "critic_function = kernel.add_function(\n",
    "    plugin_name=\"reflect\",\n",
    "    function_name=\"critic\", \n",
    "    prompt=critic_prompt, \n",
    "    prompt_execution_settings=request_settings_critic\n",
    ")\n",
    "\n",
    "@traceable(name=\"Critic Step\")\n",
    "async def run_critic(answer):\n",
    "    result = await kernel.invoke(critic_function, answer=str(answer.generated_answer))\n",
    "    return Critique.model_validate(json.loads(result.value[0].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85ead930",
   "metadata": {},
   "outputs": [],
   "source": [
    "refiner_prompt = \"\"\"\n",
    "Improve the original answer using the critique.\n",
    "\n",
    "Original Answer:\n",
    "{{$answer}}\n",
    "\n",
    "Issues Found:\n",
    "{{$issues}}\n",
    "\n",
    "Provide an improved version.\n",
    "\"\"\"\n",
    "request_settings_refiner = AzureChatPromptExecutionSettings(temperature=0,max_tokens=500,\n",
    "                                                           response_format=GenerateAnswer)\n",
    "refiner_function = kernel.add_function(plugin_name=\"reflect\", function_name=\"refiner\",\n",
    "                                       prompt=refiner_prompt,\n",
    "                                       prompt_execution_settings=request_settings_refiner)\n",
    "\n",
    "@traceable(name=\"Refiner Step\")\n",
    "async def run_refiner(answer, critique):\n",
    "    final_result = await kernel.invoke(\n",
    "        refiner_function,\n",
    "        answer=str(answer.generated_answer),\n",
    "        issues=\"\\n\".join(critique.issues)\n",
    "    )\n",
    "    return GenerateAnswer.model_validate(json.loads(final_result.value[0].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25ab5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(name=\"Reflection Agent\")\n",
    "async def reflection_agent(user_question):\n",
    "\n",
    "    # Step 1: Generate\n",
    "    draft = await run_generator(user_question)\n",
    "\n",
    "    # Step 2: Critique\n",
    "    critique = await run_critic(draft)\n",
    "\n",
    "    print(f\"[Critic Score] {critique.score}/10\")\n",
    "\n",
    "    # Step 3: Improve if needed\n",
    "    if critique.score < 8:\n",
    "        final_answer = await run_refiner(draft, critique)\n",
    "    else:\n",
    "        final_answer = draft\n",
    "\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd8d832c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Critic Score] 7/10\n"
     ]
    }
   ],
   "source": [
    "answer = await reflection_agent(\n",
    "    \"Explain how hybrid retrieval improves RAG systems\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fad34d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid retrieval significantly enhances Retrieval-Augmented Generation (RAG) systems by effectively merging the strengths of traditional information retrieval methods with advanced generative models. In a typical RAG system, the retrieval component is responsible for fetching relevant documents from a vast corpus based on a user query, while the generative model synthesizes coherent responses using the information retrieved.\n",
      "\n",
      "Hybrid retrieval employs a combination of various retrieval techniques, such as keyword-based search, which focuses on exact term matches, and semantic search, which understands the context and meaning behind the words. For instance, a hybrid system might use traditional Boolean search to quickly locate documents containing specific keywords, while simultaneously applying vector-based semantic search to identify documents that are contextually relevant, even if they do not contain the exact query terms. This dual approach not only improves the quality and relevance of the retrieved documents but also enhances the overall accuracy and informativeness of the generated responses.\n",
      "\n",
      "Moreover, hybrid retrieval can incorporate advanced techniques such as embeddings and neural networks, which allow the system to grasp the subtleties of language better. For example, using embeddings from models like BERT or GPT can help the system understand synonyms and related concepts, thereby improving the generative model's ability to produce coherent and contextually appropriate outputs.\n",
      "\n",
      "However, it is important to acknowledge some potential limitations and challenges associated with hybrid retrieval in RAG systems. For instance, the integration of multiple retrieval methods can increase computational complexity and response time, particularly if the system needs to process large datasets in real-time. Additionally, balancing the contributions of traditional and generative components can be challenging, as over-reliance on one method may lead to suboptimal performance.\n",
      "\n",
      "In summary, the synergy created by hybrid retrieval in RAG systems results in a more robust and effective framework capable of delivering high-quality answers across diverse scenarios. By leveraging both traditional and modern retrieval techniques, these systems can provide users with more accurate, relevant, and contextually rich information.\n"
     ]
    }
   ],
   "source": [
    "print(answer.generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa856ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82234e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82164be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac344e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb35d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd401d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
